services:
  api:
    build:
      context: .
      dockerfile: docker/fastapi/Dockerfile
    image: jwill9999/llama3.2-api:${VERSION:-latest}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    container_name: llama3.2-api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    networks:
    - api
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    image: jwill9999/llama3.2-ollama:${VERSION:-latest}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    container_name: llama3.2-ollama
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
    networks:
     - api

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
      - ./open-webui-config.json:/app/backend/config/custom-endpoints.json
    environment:
      - WEBUI_AUTH=False
      - ENABLE_CUSTOM_ENDPOINTS=true
    networks:
    - api

networks:
  api:
    driver: bridge

volumes:
  ollama:
     driver: local
  open-webui:
     driver: local
  vector_store:
     driver: local

