services:
  api:
    build:
      context: .
      dockerfile: docker/fastapi/Dockerfile
    image: jwill9999/llama3.2-api:${VERSION:-latest}
    container_name: llama3.2-api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    networks:
    - api
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    image: jwill9999/llama3.2-ollama:${VERSION:-latest}
    container_name: llama3.2-ollama
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
    networks:
     - api

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
      - ./open-webui-config.json:/app/backend/config/custom-endpoints.json
    environment:
      - WEBUI_AUTH=False
      - ENABLE_CUSTOM_ENDPOINTS=true
    networks:
    - api

networks:
  api:
    driver: bridge

volumes:
  ollama:
     driver: local
  open-webui:
     driver: local
  vector_store:
     driver: local

