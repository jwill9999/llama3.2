services:
  api:
    build:
      context: .
      dockerfile: docker/fastapi/Dockerfile
    image: jwill9999/llama3.2-api:${VERSION:-latest}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    container_name: llama3.2-api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    networks:
    - api
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    image: jwill9999/llama3.2-ollama:${VERSION:-latest}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    container_name: llama3.2-ollama
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
    networks:
     - api

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
      - ./open-webui-config.json:/app/backend/config/custom-endpoints.json
    environment:
      - WEBUI_AUTH=False
      - ENABLE_CUSTOM_ENDPOINTS=true
    networks:
    - api

  mcp:
      build:
        context: .
        dockerfile: mcp/Dockerfile
      ports:
        - "6274:6274"
        - "6277:6277"
      networks:
        - api

  n8n:
    image: docker.n8n.io/n8nio/n8n
    container_name: n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - WEBHOOK_URL=http://localhost:5678
      - GENERIC_TIMEZONE=Europe/London
    volumes:
      - n8n-data:/home/node/.n8n
    networks:
    - api
    restart: unless-stopped



networks:
  api:
    driver: bridge

volumes:
  ollama:
     driver: local
  open-webui:
     driver: local
  vector_store:
     driver: local
  ollama-data:
    driver: local
  n8n-data:
    driver: local

